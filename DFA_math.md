#DFA math


<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>
## Abstruct
BPは、損失関数のランドスケープで最も急降下する方向を指す勾配を提供します。FAは、異なる更新方向を提供しますが、実験結果によると、この方法は、非線形隠れユニットを持つネットワークで誤差をゼロにすることができます。これは、原理が最急降下法とは明確に異なるため、驚くべきことです。BPでは、フィードバックの重みはフォワードの重みの転置です。FAではフィードバック重みは固定されていますが、フォワード重みが適応されると、フィードバックが有用になるように、フィードバック重みの擬似逆数とほぼ一致するようになります[9]。


フィードバック・アライメントの論文[9]は、固定されたランダムなフィードバックが、漸近的に誤差をゼロにすることを証明している。そのための条件を自由に再掲すると以下のようになる。1) ネットワークは1つの隠れ層を持つ線形である。2) 入力データは、平均値がゼロ、標準偏差が1である。3) フィードバック行列BはB+B=Iを満たし、B+はBのMoore-Penrose擬似逆行列である。 4) 順方向の重みは0に初期化されている。5) 出力層の重みは、エラーを最小化するように適応される。この新しい原理を「フィードバック・アライメント原理」と呼ぼう。

フィードバック・アライメント原理を、複数の非線形隠れ層を持つネットワークにどのように適用できるかは明らかではない。9]の実験では、誤差を出力から層ごとに逆伝播させれば、より多くの層を追加できることが示されている。

次の定理は、フィードバック・アライメントの原理を説明できるメカニズムを示しています。このメカニズムは、各データポイントの更新方向が一定であると仮定した場合に、非対称なフィードバック経路が、逆伝播された勾配と順伝播された勾配を自分の勾配に合わせることで、どのように学習を行うことができるかを説明しています。


---

 kがk + 1に接続するフィードフォワード・ニューラル・ネットワークの2つの隠れ層kとk + 1が与えられる。隠れ層の活性度をhkとhk+1とする。層間の関数依存性をhk+1 = f(ak+1)、ak+1 = Whk + bとする。ここでWは重み行列、bはバイアスベクトル、f()は非線形性である。層が非ゼロの更新方向に従って更新されるとすると
δhk and δhk+1 ここで、δhkとδhk+1は各データポイントに対して一定である。負の更新方向∥δhk ∥δhk+1 ∥。
の方向は、以下のレイヤワイズ基準を最小化する。

Kを最小化すると、アライメント基準を最大化する勾配が最大となる

ここで
証明する。iを層kまたはk＋1のいずれかとする。規定の更新-δhiは、Kiを最小化するための最急降下
δhiはKiを最小化するための最急降下方向であり、積の法則とδhiの任意の偏微分がゼロであることを利用して
δhiの偏微分がゼロであることから、∥δhi∥が得られる。


Lk＞0とすると、-δhkはKk+1を最小化するための下降方向である。

ここで、δhiは0ではないので、αi＝1は正のスカラーである。δaiを次のように定義する。

δhi ⊙ f′(ai) ここでaiは層iへの入力である。再び積の法則を用いると、LkとLk+1を最大化する勾配は



グラジエントの大きさを無視すると、∂L = ∂Lk = ∂Lk+1 となる。

δWにおける∂Lkの成分を最小にすることで、間接的にLkとLk+1を最大にすることができる。

Lk＞0は、δhkとバックプロパゲーションされた勾配ckとの間の角度βが90◦以内であることを意味する。
Kkである。Kkを最小化するための勾配は、所定の更新-δhkである。
また、それぞれecos(β)=cTkδhk=Lk >0⇒|β|<90◦.Lk >0は、ckが次のようになることも示唆している。
が0でないこと、つまり下降していることを示しています。そうすると、δhkは下降する方向を向くことになり、最も急な下降線から90◦以内のベクトルは
最も急な下降方向から90◦以内のベクトルも下降方向を指すことになります。


--- 

この定理は、学習が収束するとか、どんなエラーもゼロになるとは言っていないことに注意する必要がありますが、偽の勾配がKを減らすことに成功した場合、この勾配には、整列基準Lを増やそうとする成長成分も含まれることになります。
この定理は、ニューラルネットワークの出力層と最後の隠れ層に適用することができます。誤差駆動学習を実現するためには
エラー駆動型学習を実現するためには、フィードバックループを閉じる必要があります。すると、更新方向が得られます
δhk+1 = ∂J = e and δhk = Gk(e) ここでGk(e)は出力と∂ayの隠れ層を結ぶフィードバックパスである。
の隠れた層である。


所定の更新により、hkが与えられたときの損失Jを直接最小化する。Lkが正の値になると、フィードバックによって同じ損失を減らす更新方向δhk＝Gk(e)が与えられる。この定理は、より深い層に連続して適用することができる。各層iについて、重み行列Wiは、上の層のKi+1を最小化するように更新され、同時にそれ自身の更新方向δhi＝Gi(e)を間接的に有用にする。


定理1は、大規模な非対称フィードバックパスのクラスが、平均的にLi > 0である限り、隠れた層に下降勾配の方向を提供できることを示唆している。 フィードバックパスGi(e)を選択して、重みを固定してランダムに、後ろに向かう途中ですべての層を訪問すると、FA法が得られる。直接帰還経路Gi(e)=Bieを選び、Biを固定してランダムにすると、DFA法が得られます。最初の隠れ層に接続する直接帰還経路G1(e)=B1eを選択し、その後、すべての層を訪問して進むとIFA法が得られます。実験編では、このような間接的なフィードバックでも学習が可能であることを示しています。


直接的なランダムフィードバックδhi = Gi(e) = Bieは、δhiがすべての非ゼロのeに対してゼロでないという利点がある。非ゼロのδhiは、Li > 0を達成するための必要条件です。フィードバックを静的に保つことで、トレーニング中にこの特性が維持されます。さらに、静的なフィードバックは、δhiの方向がより一定であるため、Liを最大化することが容易になります。クロスエントロピー損失が使用され、出力目標値が0または1である場合、与えられたサンプルjに対する誤差ejの符号は変化しない。つまり，Biもsign(ej)も一定なので，量Biのsign(ej)は学習中に一定になります．タスクが分類である場合、量はさらに、クラス内のすべてのサンプルに対して一定になります。また、直接ランダムなフィードバックは、誤差eの大きさに応じてのみ変化する大きさの更新方向δhiを提供します。


順方向の重みを0に初期化すると、逆伝播される誤差が0になるため、Li＝0になります。これは、最初の更新ステップでこの量をすぐに正にすることができるため、非対称フィードバックを使用する際の良い出発点のように思えます。しかし、初期条件がゼロであることは、非対称フィードバックが機能するための必須条件ではありません。実験では、悪い初期状態からスタートした場合でも、ランダムで静的な直接フィードバックは、この量を正に変え、トレーニングエラーをゼロにすることができることを示します。


FAとBPの場合、隠れた層の成長は上の層によって制限されます。上の層が飽和してしまうと、隠れ層の更新δhiはゼロになる。DFAでは、誤差eがゼロでない限り、隠れ層の更新δhiはゼロにならない。成長を制限するためには、双曲タンジェントやロジスティック・シグモイドのような潰しの効く非線形が適切と思われる。隠れた層にtanh非線形性を加えると、隠れた活性化は[-1, 1]の範囲に収まります。初期重みがゼロの場合、hiはすべてのデータポイントでゼロになります。tanh非線形性は、初期の成長をどの方向にも制限しない。実験結果は、この非線形性がDFAと相性が良いことを示しています。


ハイパーボリックタンジェント非線形性を隠れ層で使用した場合、フォワードウェイトをゼロに初期化することができます。整流型線形活性化関数（ReLU）は、バイアスと入力重みがすべてゼロの場合、このようなユニットの誤差微分がゼロになるため、初期重みがゼロの場合は動作しません。