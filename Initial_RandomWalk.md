#Random Walks: Training Very Deep Nonlinear Feed-Forward Networks with Smart Initialization
[paper](https://www.researchgate.net/publication/269935401_Random_Walks_Training_Very_Deep_Nonlinear_Feed-Forward_Networks_with_Smart_Initialization)

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>
## Abstruct
非常に深いネットワークの学習は、機械学習における重要な未解決問題です。多くの困難の1つは、バックプロパゲートされた勾配のノルムが指数関数的に成長したり減衰したりすることである。ここでは、非常に深いフィードフォワードネットワーク(FFN)の学習は、これまで考えられていたほど難しくないことを示しています。同じ行列が繰り返し適用されるリカレントネットワークとは異なり、FNNは各層で異なる行列を適用します。我々は、最初のベクトルに正しくスケーリングされたランダムな行列を連続的に適用すると、結果として得られるベクトルのノルムの対数のランダムウォークが生じることを示します。このランダムウォークの平均値はゼロで、分散は行列の適用回数に線形です。また、この分散は、ベクトルの次元に反比例します。

我々は、バックプロップ方程式が誤差に対して線形であることを観察することによって、これらのランダムな行列の結果を数百の層を持つ非常に深いFNに適用する。実際には、対数ノルムがネットワーク深さの平方根でスケールする勾配を意味しています。さらに、層の幅を大きくすることで、消失勾配の問題を軽減できることを示唆しています。これらの主張を裏付けるために、MNISTとTIMITのデータセットを最適化するための数学的分析と確率的勾配降下法を用いた実験結果が提供される。

## 1.Intro
消失勾配問題の分析における重要な仮定は、ネットワークが再通電しているということです(例えば(Hochreiter, 1991))。再現性とは、順方向の伝搬中に同じ行列を何度も適用することを意味します。また、再帰性は、時間的に逆流する際に、非常に似た行列が勾配に適用されることを意味します。線形システムの場合、分析は、行列の主要な固有値の大きさがゼロより大きいか小さいかに帰着します1。先頭の固有値の大きさがゼロより大きければ、指数関数的に成長します。先頭の固有値の大きさがゼロよりも大きい場合、指数関数的な成長が見られ、ゼロよりも小さい場合、指数関数的な減衰が見られます。先頭の固有値の大きさが正確に1である場合にのみ、非消失性のグラデーションが存在します。残念なことに、これには初期化の微調整が必要で、最適化プロセスが進むにつれ、ほぼ確実に失われてしまいます。
興味深いことに、各層でランダムに初期化された行列を持つFFNを代わりに分析すると、数学は大きく変わります。

まず、ランダムな行列を連続的に適用して得られるベクトルのノルムを数学的に分析します。また、数百層の非線形FNのバックプロパゲーション方程式について、基本的な解析結果が経験的に成り立つことをコンピュータ実験で示しました。さらに、MNISTおよびTIMITデータセットにおいて、ネットワークの深さがどのように学習誤差を減少させるかを示します。また、これらのネットワークを初期化するための基本的なヒューリスティックを紹介します。この初期化は、逆伝播された誤差のノルムの対数（log-norms）の振る舞いから、Random Walk初期化と呼んでいます。

##3.Method

したがって、gが正しく選択された場合、ノルムは平均的に一定であり、分散は行列の適用数に応じて線形にスケールします。式(4-7)で定義されるランダムウォークの例を図1に示します。



ランダムウォークによる初期化の一般的な方法は，入力分布から実際の入力ベクトルのサブセットを伝搬させることである．入力のバッチをD層に伝搬させ、誤差を評価しますが、ネットワークは初期化されたばかりなので、誤差は大きいはずです。すべてのサンプルにおけるlog(δ1)の平均値がlog(δD)とほぼ同じになるようにgを調整します。この調整にはわずかな計算時間しかかからず、簡単に自動化することができます（ただし、図2に示すような最適な値が妥当です）。
入力分布のスケーリング自体も、平均がゼロ、分散が単位となるように調整する必要があります。入力のスケーリングが悪いと、初期行列のランダム性がスケーリングの悪さを「洗い流す」前に、式(3)の微分項によるバックプロパゲーションが初期層の数だけ影響を受けることになります。同様の理由で、バックプロパゲーションエラーが最終出力層の初期化によって影響を受けるため、最終出力層の初期スケーリングを別途調整する必要があるかもしれません。つまり、ランダムウォークの初期化では、入力スケーリング（またはg1）、gD、gの3つのパラメータを調整する必要があります。最初の2つは入力や誤差の過渡的な影響を処理するためで、最後の1つはネットワーク全体を調整するためです。この3つの中で最も重要なのはgです。