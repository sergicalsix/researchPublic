#Distributed Learning of Neural Networks using Independent
Subnet Training
[paper](https://arxiv.org/pdf/1910.01526.pdf)

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>
## Abstruct
分散型機械学習（ML）では，単一マシンでの学習よりも多くの計算資源を利用できるため，学習時間を短縮できる．さらに、分散させることで、モデルを多くのマシンに分割することができ、個々のマシンのメモリをはるかに超えるような大規模なモデルを学習することができます。しかし、実際には、通信コストが高いことが原因で、分散型MLは依然として困難です。我々は、独立サブネット学習（IST）と呼ばれる分散型ニューラルネットワーク学習の新しいアプローチを提案する。ISTでは、反復ごとに、ニューラルネットワークが元のネットワークと同じ深さのサブネットワークのセットに分解され、それぞれがローカルに学習された後、様々なサブネットが交換され、プロセスが繰り返される。ISTによる学習は、標準的なデータ並列アプローチに比べて多くの利点があります。元のネットワークが独立した部分に分解されるため、通信量が減少します。この分解により、ISTは自然に「モデル並列」となり、1台のマシンに収まらないような非常に大きなモデルにも対応できるようになります。さらに、サブセットが独立しているため、通信頻度も減少します。ISTは、分散学習のデータ並列アプローチよりもはるかに低い学習時間を実現することを実験的に示しました。さらに、ISTは、標準的なアプローチでは学習できない大規模なモデルにも対応します。

##5. Related Work 
データ並列処理では、作業者間での勾配更新の通信にかかる高い帯域幅コストが問題となることがよくあります。

Quantized SGD [4, 14, 17, 24, 27, 47, 55]および Sparsified SGD [3]は、いずれもこの問題に対処しています。Quantized SGD は、非可逆圧縮を使用して勾配を量子化します。

Sparsified SGD は、最大の大きさのグラデーションを送信することで、交換のオーバーヘッドを削減します。このような手法は IST と直交しており、IST と組み合わせて使用することができます。

最近では、並列性を利用して、「YYの学習問題をXX分で解く」という、XXの値をどんどん小さくしていく論文が次々と発表されています[13, 23, 50, 58, 60-62]。

これらの方法は，大規模なバッチを使用することが多いです．大規模なバッチ学習は「シャープ・ミニマム」に収束し，一般化を妨げるということは，一般的に受け入れられていますが，まだ議論されています [16, 31, 59]．さらに、このような結果を得るためには、特殊なハードウェアを使用する博士課程のチームが必要であると考えられます。

分散ローカル SGD [40, 64, 65, 68] は、計算ノードごとに複数のローカルステップを実行した後にのみ、平均化によってパラメータを更新します。これにより、同期が減少するため、ハードウェアの効率が向上します。ISTは同様のアプローチを採用していますが、ローカルSGDと各同期ラウンドのコストを低く抑えています。

最近のアプローチ[37]では、トレーニングの最後に同期の頻度を減らすことを提案していますが、最初から同期を避けることはできません。最後に、非同期化によって SGD の同期コストが回避されます [15, 41, 45, 66]。これは、分散メモリシステムで使用されています。

このようなシステムは、漸近的には素晴らしい収束率が保証されていますが、制約のない非同期性は必ずしもうまくいかないという意見が増えてきており[11]、実際には好まれなくなってきているようです。

##6.Conclusion
本研究では，ニューラルネットワークの分散学習のための独立サブネット学習を提案する．ISTは、モデルを非重複サブネットに確率的に分割することで、モデル同期のための通信オーバーヘッドと、各ワーカー上のより薄いモデルのための前進・後退伝播の計算ワークロードを削減します。この結果、2つの進歩があります。𝑖）ISTは、分散学習のための標準的なデータ並列アプローチと比較して、学習プロセスを大幅に高速化する、および𝑖）ISTは、標準的なデータ並列アプローチでは学習できない大規模なモデルに対応する。